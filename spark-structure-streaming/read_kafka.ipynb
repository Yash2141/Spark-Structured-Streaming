{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5e2ecb0-b1e8-4184-be89-f06f06ee1e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://afcdad16a6fa:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Streaming from Kafka</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x775491db9870>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create spark session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=(\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Streaming from Kafka\")\n",
    "    .config(\"spark.streaming.stopGracefullyonshutdown\", True)\n",
    "    .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0\")\n",
    "    .config(\"spark.sql.shuffle.partitions\",4)\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    "    )\n",
    "    \n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d1f41c8-4669-4758-a890-72fa5c3df995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the kafka_df to read form kafka\n",
    "\n",
    "kafka_df = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\",\"ed-kafka:29092\")\n",
    "    .option(\"subscribe\",\"device-data\")\n",
    "    .option(\"startingoffsets\",\"earliest\")\n",
    "    .load()\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cc8a650-50e0-4e07-a1e9-c3aeb19d0e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n",
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "| key|               value|      topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "|null|                  []|device-data|        0|     0|2026-02-03 12:21:...|            0|\n",
      "|null|[7B 22 65 76 65 6...|device-data|        0|     1|2026-02-03 12:21:...|            0|\n",
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view  schema for raw kafka_df\n",
    "\n",
    "kafka_df.printSchema()\n",
    "kafka_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "891e0fbc-9b25-47c1-8244-eb371643337e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse value from binary to string into kafka_json_df\n",
    "\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "kafka_json_df = kafka_df.withColumn(\"value\",expr(\"cast(value as string)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d6a21d4-6fce-4b57-90a8-f4fe67cb3d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "| key|               value|      topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "|null|                    |device-data|        0|     0|2026-02-03 12:21:...|            0|\n",
      "|null|{\"eventId\": \"ba2e...|device-data|        0|     1|2026-02-03 12:21:...|            0|\n",
      "+----+--------------------+-----------+---------+------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_json_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29243506-36c4-4da4-a02f-2e39bb6b8069",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import StringType, StructField, StructType, ArrayType,  LongType\n",
    "\n",
    "json_schema = (\n",
    "    StructType(\n",
    "    [StructField('customerId',StringType(), True),\n",
    "    StructField('data', StructType(\n",
    "        [StructField('devices',\n",
    "                    ArrayType(StructType([\n",
    "                    StructField('deviceId', StringType(), True),\n",
    "                    StructField('measure', StringType(), True),\n",
    "                    StructField('status', StringType(), True),\n",
    "                    StructField('temperature',LongType(), True)\n",
    "                ]), True), True)\n",
    "        ]), True),\n",
    "StructField('eventId', StringType(), True),\n",
    "StructField('eventoffset', LongType(), True),\n",
    "StructField('eventPublisher', StringType(), True),\n",
    "StructField('eventTime', StringType(), True)\n",
    "])\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0478f4a-4e76-4960-9ea0-c85ba18ed9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply the schema to payload to read the data\n",
    "\n",
    "from pyspark.sql.functions import from_json, col\n",
    "\n",
    "streaming_df = kafka_json_df.withColumn(\"values_json\", from_json(col(\"value\"), json_schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38f09d66-4588-417d-946c-907df28c1e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      " |-- values_json: struct (nullable = true)\n",
      " |    |-- customerId: string (nullable = true)\n",
      " |    |-- data: struct (nullable = true)\n",
      " |    |    |-- devices: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- deviceId: string (nullable = true)\n",
      " |    |    |    |    |-- measure: string (nullable = true)\n",
      " |    |    |    |    |-- status: string (nullable = true)\n",
      " |    |    |    |    |-- temperature: long (nullable = true)\n",
      " |    |-- eventId: string (nullable = true)\n",
      " |    |-- eventoffset: long (nullable = true)\n",
      " |    |-- eventPublisher: string (nullable = true)\n",
      " |    |-- eventTime: string (nullable = true)\n",
      "\n",
      "+----+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+---------+------+-----------------------+-------------+-----------------------------------------------------------------------------------------------+\n",
      "|key |value                                                                                                                                                                                             |topic      |partition|offset|timestamp              |timestampType|values_json                                                                                    |\n",
      "+----+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+---------+------+-----------------------+-------------+-----------------------------------------------------------------------------------------------+\n",
      "|null|                                                                                                                                                                                                  |device-data|0        |0     |2026-02-03 12:21:46.271|0            |null                                                                                           |\n",
      "|null|{\"eventId\": \"ba2ea9f4-a5d9-434e-8e4d-1c80c2d4b456\", \"eventOffset\": 10000, \"eventPublisher\": \"device\", \"customerId\": \"CI00119\", \"data\": {\"devices\": []}, \"eventTime\": \"2023-01-05 11:13:53.643364\"}|device-data|0        |1     |2026-02-03 12:21:48.598|0            |{CI00119, {[]}, ba2ea9f4-a5d9-434e-8e4d-1c80c2d4b456, null, device, 2023-01-05 11:13:53.643364}|\n",
      "+----+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+---------+------+-----------------------+-------------+-----------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To the schema of the data, place a sample json file and change readStream to read \n",
    "\n",
    "streaming_df.printSchema()\n",
    "streaming_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "076f66d7-9046-4daf-96ca-78da11547238",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_df = kafka_json_df.withColumn(\"values_json\", from_json(col(\"value\"), json_schema)).selectExpr(\"values_json.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7742c426-102d-4a1b-9e5e-5371da2a41b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerId: string (nullable = true)\n",
      " |-- data: struct (nullable = true)\n",
      " |    |-- devices: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- deviceId: string (nullable = true)\n",
      " |    |    |    |-- measure: string (nullable = true)\n",
      " |    |    |    |-- status: string (nullable = true)\n",
      " |    |    |    |-- temperature: long (nullable = true)\n",
      " |-- eventId: string (nullable = true)\n",
      " |-- eventoffset: long (nullable = true)\n",
      " |-- eventPublisher: string (nullable = true)\n",
      " |-- eventTime: string (nullable = true)\n",
      "\n",
      "+----------+----+------------------------------------+-----------+--------------+--------------------------+\n",
      "|customerId|data|eventId                             |eventoffset|eventPublisher|eventTime                 |\n",
      "+----------+----+------------------------------------+-----------+--------------+--------------------------+\n",
      "|null      |null|null                                |null       |null          |null                      |\n",
      "|CI00119   |{[]}|ba2ea9f4-a5d9-434e-8e4d-1c80c2d4b456|null       |device        |2023-01-05 11:13:53.643364|\n",
      "+----------+----+------------------------------------+-----------+--------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streaming_df.printSchema()\n",
    "streaming_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dfd1d6ba-7f75-4d83-8ea9-47a239901516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets explode the data as devices contains List/array of device reading\n",
    "\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "explode_df = streaming_df.withColumn(\"data_devices\", explode(\"data.devices\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52f2f4d0-c695-4341-b639-a2b8c5bd911d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerId: string (nullable = true)\n",
      " |-- data: struct (nullable = true)\n",
      " |    |-- devices: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- deviceId: string (nullable = true)\n",
      " |    |    |    |-- measure: string (nullable = true)\n",
      " |    |    |    |-- status: string (nullable = true)\n",
      " |    |    |    |-- temperature: long (nullable = true)\n",
      " |-- eventId: string (nullable = true)\n",
      " |-- eventoffset: long (nullable = true)\n",
      " |-- eventPublisher: string (nullable = true)\n",
      " |-- eventTime: string (nullable = true)\n",
      " |-- data_devices: struct (nullable = true)\n",
      " |    |-- deviceId: string (nullable = true)\n",
      " |    |-- measure: string (nullable = true)\n",
      " |    |-- status: string (nullable = true)\n",
      " |    |-- temperature: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the schema of the explode_df, place a sample json file and change readStream to read\n",
    "\n",
    "explode_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "055b7b14-ef30-4464-a2c0-3404936ddf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the explode df\n",
    "from pyspark.sql.functions import col\n",
    " \n",
    "flattened_df = (\n",
    "    explode_df\n",
    "    .drop(\"data\")\n",
    "    .withColumn(\"deviceId\", col(\"data_devices.deviceId\"))\n",
    "    .withColumn(\"measure\", col(\"data_devices.measure\"))\n",
    "    .withColumn(\"status\", col(\"data_devices.status\"))\n",
    "    .withColumn(\"temperature\", col(\"data_devices.temperature\"))\n",
    "    .drop(\"data_devices\")\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2bbb3dc4-85c8-4335-bb34-470d1e6dc43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerId: string (nullable = true)\n",
      " |-- eventId: string (nullable = true)\n",
      " |-- eventoffset: long (nullable = true)\n",
      " |-- eventPublisher: string (nullable = true)\n",
      " |-- eventTime: string (nullable = true)\n",
      " |-- deviceId: string (nullable = true)\n",
      " |-- measure: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- temperature: long (nullable = true)\n",
      "\n",
      "+----------+-------+-----------+--------------+---------+--------+-------+------+-----------+\n",
      "|customerId|eventId|eventoffset|eventPublisher|eventTime|deviceId|measure|status|temperature|\n",
      "+----------+-------+-----------+--------------+---------+--------+-------+------+-----------+\n",
      "+----------+-------+-----------+--------------+---------+--------+-------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the schema of the flattened_df, place a sample json file and change readStream to read\n",
    "\n",
    "flattened_df.printSchema()\n",
    "flattened_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3fc125-3a27-436f-832c-c7a58539d9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4647b2c-0f3c-4962-8227-0759b7553a14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc60f92e-0741-4c51-82f2-da934f4e1462",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e5b1d914-2149-4025-9d7c-df5857c7d0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the kafka_df to read form kafka stream\n",
    "\n",
    "kafka_df = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\",\"ed-kafka:29092\")\n",
    "    .option(\"subscribe\",\"device-data\")\n",
    "    .option(\"startingoffsets\",\"earliest\")\n",
    "    .load()\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b031ab7d-13f0-4b59-bb0d-374d6879514b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4d516d83-66d4-4ecd-aa15-a14d1243d64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse value from binary to string into kafka_json_df\n",
    "\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "kafka_json_df = kafka_df.withColumn(\"value\",expr(\"cast(value as string)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8201cd30-e9ba-4623-992c-9bfb9b9e5dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import StringType, StructField, StructType, ArrayType,  LongType\n",
    "\n",
    "json_schema = (\n",
    "    StructType(\n",
    "    [StructField('customerId',StringType(), True),\n",
    "    StructField('data', StructType(\n",
    "        [StructField('devices',\n",
    "                    ArrayType(StructType([\n",
    "                    StructField('deviceId', StringType(), True),\n",
    "                    StructField('measure', StringType(), True),\n",
    "                    StructField('status', StringType(), True),\n",
    "                    StructField('temperature',LongType(), True)\n",
    "                ]), True), True)\n",
    "        ]), True),\n",
    "StructField('eventId', StringType(), True),\n",
    "StructField('eventoffset', LongType(), True),\n",
    "StructField('eventPublisher', StringType(), True),\n",
    "StructField('eventTime', StringType(), True)\n",
    "])\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9c39699f-98e0-4999-823a-1e310d1f50a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply the schema to payload to read the data\n",
    "\n",
    "from pyspark.sql.functions import from_json, col\n",
    "\n",
    "streaming_df = kafka_json_df.withColumn(\"values_json\", from_json(col(\"value\"), json_schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "93c64b2e-ef35-4652-ae68-cfa6f394a52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      " |-- values_json: struct (nullable = true)\n",
      " |    |-- customerId: string (nullable = true)\n",
      " |    |-- data: struct (nullable = true)\n",
      " |    |    |-- devices: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- deviceId: string (nullable = true)\n",
      " |    |    |    |    |-- measure: string (nullable = true)\n",
      " |    |    |    |    |-- status: string (nullable = true)\n",
      " |    |    |    |    |-- temperature: long (nullable = true)\n",
      " |    |-- eventId: string (nullable = true)\n",
      " |    |-- eventoffset: long (nullable = true)\n",
      " |    |-- eventPublisher: string (nullable = true)\n",
      " |    |-- eventTime: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To the schema of the data, place a sample json file and change readStream to read \n",
    "\n",
    "streaming_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b29d2be0-91bc-417a-b174-5e7cf293c25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_df = kafka_json_df.withColumn(\"values_json\", from_json(col(\"value\"), json_schema)).selectExpr(\"values_json.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bc12faf2-c18f-42b4-a6ef-261d526ef51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerId: string (nullable = true)\n",
      " |-- data: struct (nullable = true)\n",
      " |    |-- devices: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- deviceId: string (nullable = true)\n",
      " |    |    |    |-- measure: string (nullable = true)\n",
      " |    |    |    |-- status: string (nullable = true)\n",
      " |    |    |    |-- temperature: long (nullable = true)\n",
      " |-- eventId: string (nullable = true)\n",
      " |-- eventoffset: long (nullable = true)\n",
      " |-- eventPublisher: string (nullable = true)\n",
      " |-- eventTime: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streaming_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "564c0af6-c54d-460b-b282-69ebb756bb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerId: string (nullable = true)\n",
      " |-- data: struct (nullable = true)\n",
      " |    |-- devices: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- deviceId: string (nullable = true)\n",
      " |    |    |    |-- measure: string (nullable = true)\n",
      " |    |    |    |-- status: string (nullable = true)\n",
      " |    |    |    |-- temperature: long (nullable = true)\n",
      " |-- eventId: string (nullable = true)\n",
      " |-- eventoffset: long (nullable = true)\n",
      " |-- eventPublisher: string (nullable = true)\n",
      " |-- eventTime: string (nullable = true)\n",
      " |-- data_devices: struct (nullable = true)\n",
      " |    |-- deviceId: string (nullable = true)\n",
      " |    |-- measure: string (nullable = true)\n",
      " |    |-- status: string (nullable = true)\n",
      " |    |-- temperature: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets explode the data as devices contains List/array of device reading\n",
    "\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "explode_df = streaming_df.withColumn(\"data_devices\", explode(\"data.devices\"))\n",
    "\n",
    "explode_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "babae0b6-3230-4fd5-909f-614d01af39be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerId: string (nullable = true)\n",
      " |-- eventId: string (nullable = true)\n",
      " |-- eventoffset: long (nullable = true)\n",
      " |-- eventPublisher: string (nullable = true)\n",
      " |-- eventTime: string (nullable = true)\n",
      " |-- deviceId: string (nullable = true)\n",
      " |-- measure: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- temperature: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# flatten the explode df\n",
    "from pyspark.sql.functions import col\n",
    " \n",
    "flattened_df = (\n",
    "    explode_df\n",
    "    .drop(\"data\")\n",
    "    .withColumn(\"deviceId\", col(\"data_devices.deviceId\"))\n",
    "    .withColumn(\"measure\", col(\"data_devices.measure\"))\n",
    "    .withColumn(\"status\", col(\"data_devices.status\"))\n",
    "    .withColumn(\"temperature\", col(\"data_devices.temperature\"))\n",
    "    .drop(\"data_devices\")\n",
    "\n",
    ")\n",
    "\n",
    "flattened_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca1b8df-c7e6-43b7-a242-beff03e19af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # write the output to console to check the output\n",
    "\n",
    "(flattened_df\n",
    ".writeStream\n",
    ".format(\"console\")\n",
    ".outputMode(\"append\")\n",
    ".option(\"checkpointlocation\",\"checkpoint_dir_kafka\")\n",
    ".start()\n",
    ".awaitTermination())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6223fb25-d661-4c2a-8f3b-01771268cddc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
