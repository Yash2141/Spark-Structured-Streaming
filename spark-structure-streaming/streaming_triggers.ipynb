{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ce668e5-5776-40c3-9e18-48bc109abb2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://3af5122759b6:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Triggers in Spark Streaming</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x72febc2f22f0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create spark session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=(\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Triggers in Spark Streaming\")\n",
    "    .config(\"spark.streaming.stopGracefullyonshutdown\", True)\n",
    "    .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0\")\n",
    "    .config(\"spark.sql.shuffle.partitions\",4)\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    "    )\n",
    "    \n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efab7b2c-8382-4dc2-b8b2-bdae24496be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the kafka_df to read form kafka\n",
    "\n",
    "kafka_df = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\",\"ed-kafka:29092\")\n",
    "    .option(\"subscribe\",\"device-data\")\n",
    "    .option(\"startingoffsets\",\"earliest\")\n",
    "    .load()\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2331daf9-bb9d-4b5f-8b1e-871082404e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view  schema for raw kafka_df\n",
    "\n",
    "kafka_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cdf06c7b-c90d-40f4-a332-7df6c9146091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse value from binary to string into kafka_json_df\n",
    "\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "kafka_json_df = kafka_df.withColumn(\"value\",expr(\"cast(value as string)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3673f768-e187-4651-ac41-03e7094e156a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import StringType, StructField, StructType, ArrayType,  LongType\n",
    "\n",
    "json_schema = (\n",
    "    StructType(\n",
    "    [StructField('customerId',StringType(), True),\n",
    "    StructField('data', StructType(\n",
    "        [StructField('devices',\n",
    "                    ArrayType(StructType([\n",
    "                    StructField('deviceId', StringType(), True),\n",
    "                    StructField('measure', StringType(), True),\n",
    "                    StructField('status', StringType(), True),\n",
    "                    StructField('temperature',LongType(), True)\n",
    "                ]), True), True)\n",
    "        ]), True),\n",
    "StructField('eventId', StringType(), True),\n",
    "StructField('eventoffset', LongType(), True),\n",
    "StructField('eventPublisher', StringType(), True),\n",
    "StructField('eventTime', StringType(), True)\n",
    "])\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2868dcc4-1088-455a-888b-22f1df60430d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply the schema to payload to read the data\n",
    "\n",
    "from pyspark.sql.functions import from_json, col\n",
    "\n",
    "streaming_df = kafka_json_df.withColumn(\"values_json\", from_json(col(\"value\"), json_schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24897e76-3325-47dc-8f48-d605719e8d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      " |-- values_json: struct (nullable = true)\n",
      " |    |-- customerId: string (nullable = true)\n",
      " |    |-- data: struct (nullable = true)\n",
      " |    |    |-- devices: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- deviceId: string (nullable = true)\n",
      " |    |    |    |    |-- measure: string (nullable = true)\n",
      " |    |    |    |    |-- status: string (nullable = true)\n",
      " |    |    |    |    |-- temperature: long (nullable = true)\n",
      " |    |-- eventId: string (nullable = true)\n",
      " |    |-- eventoffset: long (nullable = true)\n",
      " |    |-- eventPublisher: string (nullable = true)\n",
      " |    |-- eventTime: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To the schema of the data, place a sample json file and change readStream to read \n",
    "\n",
    "streaming_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f230d74a-4048-4d04-94b7-e538f532e501",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_df = kafka_json_df.withColumn(\"values_json\", from_json(col(\"value\"), json_schema)).selectExpr(\"values_json.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26614e04-cba7-482c-9359-0250cfdeb582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerId: string (nullable = true)\n",
      " |-- data: struct (nullable = true)\n",
      " |    |-- devices: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- deviceId: string (nullable = true)\n",
      " |    |    |    |-- measure: string (nullable = true)\n",
      " |    |    |    |-- status: string (nullable = true)\n",
      " |    |    |    |-- temperature: long (nullable = true)\n",
      " |-- eventId: string (nullable = true)\n",
      " |-- eventoffset: long (nullable = true)\n",
      " |-- eventPublisher: string (nullable = true)\n",
      " |-- eventTime: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streaming_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f5431e9-8f48-4789-9eba-d27fc3ee0f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerId: string (nullable = true)\n",
      " |-- data: struct (nullable = true)\n",
      " |    |-- devices: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- deviceId: string (nullable = true)\n",
      " |    |    |    |-- measure: string (nullable = true)\n",
      " |    |    |    |-- status: string (nullable = true)\n",
      " |    |    |    |-- temperature: long (nullable = true)\n",
      " |-- eventId: string (nullable = true)\n",
      " |-- eventoffset: long (nullable = true)\n",
      " |-- eventPublisher: string (nullable = true)\n",
      " |-- eventTime: string (nullable = true)\n",
      " |-- data_devices: struct (nullable = true)\n",
      " |    |-- deviceId: string (nullable = true)\n",
      " |    |-- measure: string (nullable = true)\n",
      " |    |-- status: string (nullable = true)\n",
      " |    |-- temperature: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets explode the data as devices contains List/array of device reading\n",
    "\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "explode_df = streaming_df.withColumn(\"data_devices\", explode(\"data.devices\"))\n",
    "\n",
    "explode_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c02ab2f6-d8cb-49b3-ad5d-14e31ee5eaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerId: string (nullable = true)\n",
      " |-- eventId: string (nullable = true)\n",
      " |-- eventoffset: long (nullable = true)\n",
      " |-- eventPublisher: string (nullable = true)\n",
      " |-- eventTime: string (nullable = true)\n",
      " |-- deviceId: string (nullable = true)\n",
      " |-- measure: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- temperature: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# flatten the explode df\n",
    "from pyspark.sql.functions import col\n",
    " \n",
    "flattened_df = (\n",
    "    explode_df\n",
    "    .drop(\"data\")\n",
    "    .withColumn(\"deviceId\", col(\"data_devices.deviceId\"))\n",
    "    .withColumn(\"measure\", col(\"data_devices.measure\"))\n",
    "    .withColumn(\"status\", col(\"data_devices.status\"))\n",
    "    .withColumn(\"temperature\", col(\"data_devices.temperature\"))\n",
    "    .drop(\"data_devices\")\n",
    "\n",
    ")\n",
    "\n",
    "flattened_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08b5132b-659c-431d-bad4-d92f9b2744d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'writeStream' can be called only on streaming Dataset/DataFrame",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 16\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Running in once/availableNow and processingTime mode\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# write the output to console sink to check the output\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# .start()\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# .awaitTermination())\u001b[39;00m\n\u001b[1;32m     14\u001b[0m (\n\u001b[1;32m     15\u001b[0m     \u001b[43mflattened_df\u001b[49m\n\u001b[0;32m---> 16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteStream\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsole\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;241m.\u001b[39mtrigger(once\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)   \u001b[38;5;66;03m# or availableNow=True (Spark 3.5+)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtruncate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpointLocation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint_dir_kafka_1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;241m.\u001b[39mawaitTermination()\n\u001b[1;32m     24\u001b[0m )\n",
      "File \u001b[0;32m/spark/python/pyspark/sql/dataframe.py:356\u001b[0m, in \u001b[0;36mDataFrame.writeStream\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwriteStream\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataStreamWriter:\n\u001b[1;32m    342\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03m    Interface for saving the content of the streaming :class:`DataFrame` out into external\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;124;03m    storage.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;124;03m    :class:`DataStreamWriter`\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataStreamWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/spark/python/pyspark/sql/streaming.py:860\u001b[0m, in \u001b[0;36mDataStreamWriter.__init__\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df \u001b[38;5;241m=\u001b[39m df\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39msparkSession\n\u001b[0;32m--> 860\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteStream\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/spark/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'writeStream' can be called only on streaming Dataset/DataFrame"
     ]
    }
   ],
   "source": [
    "# Running in once/availableNow and processingTime mode\n",
    "# write the output to console sink to check the output\n",
    "\n",
    "(\n",
    "    flattened_df\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"append\")\n",
    "    .trigger(once=True)   # or availableNow=True (Spark 3.5+)\n",
    "    .option(\"truncate\", \"false\")\n",
    "    .option(\"checkpointLocation\", \"checkpoint_dir_kafka_1\")\n",
    "    .start()\n",
    "    .awaitTermination()\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786b073c-3318-48ee-92ac-b0792f89e834",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    flattened_df\n",
    "    .writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"append\")\n",
    "    .trigger(once=True)   # or availableNow=True (Spark 3.5+)\n",
    "    .option(\"truncate\", \"false\")\n",
    "    .option(\"checkpointLocation\", \"checkpoint_dir_kafka_1\")\n",
    "    .start()\n",
    "    .awaitTermination()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a288717-96a7-48a3-ac00-3701265fc616",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
