{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50f17928-09d2-4b20-81f9-cca182f607da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://afcdad16a6fa:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Streaming Process Files</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ae912386650>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Spark Session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = ( \n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Streaming Process Files\")\n",
    "    .config(\"spark.streaming.stopGracefullyonshutdown\", True)\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "117f43de-0ad7-4bc8-98ad-920df96c146b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To allow automatic schemaInference while reading\n",
    "# spark.conf.set(\"spark.sql.streaming.schemaInference\", True)\n",
    "streaming_df = (\n",
    "    spark.read\n",
    "    .format(\"json\")\n",
    "    .load(\"/home/jupyter/streaming-spark/data/input/device_file/device_01.json\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97cf916b-2a06-42de-8883-3a1235e8ba0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerId: string (nullable = true)\n",
      " |-- data: struct (nullable = true)\n",
      " |    |-- devices: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- deviceId: string (nullable = true)\n",
      " |    |    |    |-- measure: string (nullable = true)\n",
      " |    |    |    |-- status: string (nullable = true)\n",
      " |    |    |    |-- temperature: long (nullable = true)\n",
      " |-- eventId: string (nullable = true)\n",
      " |-- eventOffset: long (nullable = true)\n",
      " |-- eventPublisher: string (nullable = true)\n",
      " |-- eventTime: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To the schema of the data, place a sample json file and change readStream to read\n",
    "\n",
    "streaming_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5338345c-5c6a-4bea-993d-dd935f80d8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets explode the data as devices contains List/array of device reading\n",
    "\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "explode_df = streaming_df.withColumn(\"data_devices\", explode(\"data.devices\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "341663b7-9a25-4d03-b3c7-dad6591fe729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerId: string (nullable = true)\n",
      " |-- data: struct (nullable = true)\n",
      " |    |-- devices: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- deviceId: string (nullable = true)\n",
      " |    |    |    |-- measure: string (nullable = true)\n",
      " |    |    |    |-- status: string (nullable = true)\n",
      " |    |    |    |-- temperature: long (nullable = true)\n",
      " |-- eventId: string (nullable = true)\n",
      " |-- eventOffset: long (nullable = true)\n",
      " |-- eventPublisher: string (nullable = true)\n",
      " |-- eventTime: string (nullable = true)\n",
      " |-- data_devices: struct (nullable = true)\n",
      " |    |-- deviceId: string (nullable = true)\n",
      " |    |-- measure: string (nullable = true)\n",
      " |    |-- status: string (nullable = true)\n",
      " |    |-- temperature: long (nullable = true)\n",
      "\n",
      "+----------+------------------------------------------------+------------------------------------+-----------+--------------+--------------------------+----------------------+\n",
      "|customerId|data                                            |eventId                             |eventOffset|eventPublisher|eventTime                 |data_devices          |\n",
      "+----------+------------------------------------------------+------------------------------------+-----------+--------------+--------------------------+----------------------+\n",
      "|CI00103   |{[{D001, C, ERROR, 15}, {D002, C, SUCCESS, 16}]}|e3cb26d3-41b2-49a2-84f3-0156ed8d7502|10001      |device        |2023-01-05 11:13:53.643364|{D001, C, ERROR, 15}  |\n",
      "|CI00103   |{[{D001, C, ERROR, 15}, {D002, C, SUCCESS, 16}]}|e3cb26d3-41b2-49a2-84f3-0156ed8d7502|10001      |device        |2023-01-05 11:13:53.643364|{D002, C, SUCCESS, 16}|\n",
      "+----------+------------------------------------------------+------------------------------------+-----------+--------------+--------------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the schema of the explode_df, place a sample json file and change readStream to read\n",
    "\n",
    "explode_df.printSchema()\n",
    "explode_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b54ba23d-d1c2-4126-af4a-8b035c9506de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the explode df\n",
    "from pyspark.sql.functions import col\n",
    " \n",
    "flattened_df = (\n",
    "    explode_df\n",
    "    .drop(\"data\")\n",
    "    .withColumn(\"deviceId\", col(\"data_devices.deviceId\"))\n",
    "    .withColumn(\"measure\", col(\"data_devices.measure\"))\n",
    "    .withColumn(\"status\", col(\"data_devices.status\"))\n",
    "    .withColumn(\"temperature\", col(\"data_devices.temperature\"))\n",
    "    .drop(\"data_devices\")\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3247db3e-66e6-429e-a081-63f15d097434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerId: string (nullable = true)\n",
      " |-- eventId: string (nullable = true)\n",
      " |-- eventOffset: long (nullable = true)\n",
      " |-- eventPublisher: string (nullable = true)\n",
      " |-- eventTime: string (nullable = true)\n",
      " |-- deviceId: string (nullable = true)\n",
      " |-- measure: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- temperature: long (nullable = true)\n",
      "\n",
      "+----------+------------------------------------+-----------+--------------+--------------------------+--------+-------+-------+-----------+\n",
      "|customerId|eventId                             |eventOffset|eventPublisher|eventTime                 |deviceId|measure|status |temperature|\n",
      "+----------+------------------------------------+-----------+--------------+--------------------------+--------+-------+-------+-----------+\n",
      "|CI00103   |e3cb26d3-41b2-49a2-84f3-0156ed8d7502|10001      |device        |2023-01-05 11:13:53.643364|D001    |C      |ERROR  |15         |\n",
      "|CI00103   |e3cb26d3-41b2-49a2-84f3-0156ed8d7502|10001      |device        |2023-01-05 11:13:53.643364|D002    |C      |SUCCESS|16         |\n",
      "+----------+------------------------------------+-----------+--------------+--------------------------+--------+-------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the schema of the flattened_df, place a sample json file and change readStream to read\n",
    "\n",
    "flattened_df.printSchema()\n",
    "flattened_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1259b815-6fe3-4908-8d83-e858930fd0fc",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "678dccd1-584f-4981-adac-07ac3b62e02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To allow automatic schemaInference while reading\n",
    "spark.conf.set(\"spark.sql.streaming.schemaInference\", True)\n",
    "streaming_df = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .option(\"cleanSource\",\"archive\")\n",
    "    .option(\"sourceArchiveDir\",\"archive_dir\")\n",
    "    .option(\"maxFilesPerTrigger\",1)\n",
    "    .format(\"json\")\n",
    "    .load(\"/home/jupyter/streaming-spark/data/input/device_file/device_01.json\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "272e94e2-9d13-42a6-b720-e85de261e233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerId: string (nullable = true)\n",
      " |-- data: struct (nullable = true)\n",
      " |    |-- devices: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- deviceId: string (nullable = true)\n",
      " |    |    |    |-- measure: string (nullable = true)\n",
      " |    |    |    |-- status: string (nullable = true)\n",
      " |    |    |    |-- temperature: long (nullable = true)\n",
      " |-- eventId: string (nullable = true)\n",
      " |-- eventOffset: long (nullable = true)\n",
      " |-- eventPublisher: string (nullable = true)\n",
      " |-- eventTime: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To the schema of the data, place a sample json file and change readStream to read\n",
    "\n",
    "streaming_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9daac9a2-3fa5-4048-934a-78ffec92d2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets explode the data as devices contains List/array of device reading\n",
    "\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "explode_df = streaming_df.withColumn(\"data_devices\", explode(\"data.devices\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3671e998-8d76-4252-b49b-c885279f5156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerId: string (nullable = true)\n",
      " |-- data: struct (nullable = true)\n",
      " |    |-- devices: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- deviceId: string (nullable = true)\n",
      " |    |    |    |-- measure: string (nullable = true)\n",
      " |    |    |    |-- status: string (nullable = true)\n",
      " |    |    |    |-- temperature: long (nullable = true)\n",
      " |-- eventId: string (nullable = true)\n",
      " |-- eventOffset: long (nullable = true)\n",
      " |-- eventPublisher: string (nullable = true)\n",
      " |-- eventTime: string (nullable = true)\n",
      " |-- data_devices: struct (nullable = true)\n",
      " |    |-- deviceId: string (nullable = true)\n",
      " |    |-- measure: string (nullable = true)\n",
      " |    |-- status: string (nullable = true)\n",
      " |    |-- temperature: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the schema of the explode_df, place a sample json file and change readStream to read\n",
    "\n",
    "explode_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67bff0c9-f2bb-4e69-ad7b-b46ea38d916f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the explode df\n",
    "from pyspark.sql.functions import col\n",
    " \n",
    "flattened_df = (\n",
    "    explode_df\n",
    "    .drop(\"data\")\n",
    "    .withColumn(\"deviceId\", col(\"data_devices.deviceId\"))\n",
    "    .withColumn(\"measure\", col(\"data_devices.measure\"))\n",
    "    .withColumn(\"status\", col(\"data_devices.status\"))\n",
    "    .withColumn(\"temperature\", col(\"data_devices.temperature\"))\n",
    "    .drop(\"data_devices\")\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b0812ba-c14c-4444-b1e6-2f9dc14089a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerId: string (nullable = true)\n",
      " |-- eventId: string (nullable = true)\n",
      " |-- eventOffset: long (nullable = true)\n",
      " |-- eventPublisher: string (nullable = true)\n",
      " |-- eventTime: string (nullable = true)\n",
      " |-- deviceId: string (nullable = true)\n",
      " |-- measure: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- temperature: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the schema of the flattened_df, place a sample json file and change readStream to read\n",
    "\n",
    "flattened_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4e24393f-5928-4af6-a9c4-2e0d8ee71ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # write the output to console to check the output\n",
    "\n",
    "(flattened_df\n",
    ".writeStream\n",
    ".format(\"csv\")\n",
    ".outputMode(\"append\")\n",
    ".option(\"path\",\"data/output/device_data.csv\")\n",
    ".option(\"checkpointlocation\",\"checkpoint_dir\")\n",
    ".start()\n",
    ".awaitTermination())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17716116-a799-4a2a-95d3-073994cfb488",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
